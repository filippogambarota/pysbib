@ARTICLE{Spence2016-tz,
  title = {Prediction interval: What to expect when you're expecting … A
  replication},
  author = {Spence, Jeffrey R and Stanley, David J},
  journaltitle = {PloS one},
  publisher = {Public Library of Science (PLoS)},
  volume = {11},
  issue = {9},
  pages = {e0162874},
  date = {2016-09-19},
  doi = {10.1371/journal.pone.0162874},
  pmc = {PMC5028066},
  pmid = {27644090},
  issn = {1932-6203},
  abstract = {A challenge when interpreting replications is determining whether
  the results of a replication "successfully" replicate the original study.
  Looking for consistency between two studies is challenging because individual
  studies are susceptible to many sources of error that can cause study results
  to deviate from each other and the population effect in unpredictable
  directions and magnitudes. In the current paper, we derive methods to compute
  a prediction interval, a range of results that can be expected in a
  replication due to chance (i.e., sampling error), for means and commonly used
  indexes of effect size: correlations and d-values. The prediction interval is
  calculable based on objective study characteristics (i.e., effect size of the
  original study and sample sizes of the original study and planned replication)
  even when sample sizes across studies are unequal. The prediction interval
  provides an a priori method for assessing if the difference between an
  original and replication result is consistent with what can be expected due to
  sample error alone. We provide open-source software tools that allow
  researchers, reviewers, replicators, and editors to easily calculate
  prediction intervals.},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0162874},
  file = {Spence and Stanley 2016 - Prediction interval - What to expect when you're expecting … A replication.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Ly2019-ow,
  title = {Replication Bayes factors from evidence updating},
  author = {Ly, Alexander and Etz, Alexander and Marsman, Maarten and
  Wagenmakers, Eric-Jan},
  journaltitle = {Behavior research methods},
  publisher = {Springer Science and Business Media LLC},
  volume = {51},
  issue = {6},
  pages = {2498-2508},
  date = {2019-12},
  doi = {10.3758/s13428-018-1092-x},
  pmc = {PMC6877488},
  pmid = {30105445},
  issn = {1554-351X,1554-3528},
  abstract = {We describe a general method that allows experimenters to quantify
  the evidence from the data of a direct replication attempt given data already
  acquired from an original study. These so-called replication Bayes factors are
  a reconceptualization of the ones introduced by Verhagen and Wagenmakers
  (Journal of Experimental Psychology: General, 143(4), 1457-1475 2014) for the
  common t test. This reconceptualization is computationally simpler and
  generalizes easily to most common experimental designs for which Bayes factors
  are available.},
  url = {http://dx.doi.org/10.3758/s13428-018-1092-x},
  file = {Ly et al. 2019 - Replication Bayes factors from evidence updating.pdf},
  keywords = {Evidence synthesis; Hypothesis testing; Meta-analysis;
  Replication;replication-methods},
  language = {en}
}

@ARTICLE{Brandt2014-da,
  title = {The Replication Recipe: What makes for a convincing replication?},
  author = {Brandt, Mark J and IJzerman, Hans and Dijksterhuis, Ap and Farach,
  Frank J and Geller, Jason and Giner-Sorolla, Roger and Grange, James A and
  Perugini, Marco and Spies, Jeffrey R and van 't Veer, Anna},
  journaltitle = {Journal of experimental social psychology},
  publisher = {Elsevier BV},
  volume = {50},
  pages = {217-224},
  date = {2014-01},
  doi = {10.1016/j.jesp.2013.10.005},
  issn = {0022-1031,1096-0465},
  abstract = {Psychological scientists have recently started to reconsider the
  importance of close replications in building a cumulative knowledge base;
  however, there is no consensus about what constitutes a convincing close
  replication study. To facilitate convincing close replication attempts we have
  developed a Replication Recipe, outlining standard criteria for a convincing
  close replication. Our Replication Recipe can be used by researchers,
  teachers, and students to conduct meaningful replication studies and integrate
  replications into their scholarly habits.},
  url = {https://www.sciencedirect.com/science/article/pii/S0022103113001819},
  file = {Brandt et al. 2014 - The Replication Recipe - What makes for a convincing replication.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Hedges2019-ar,
  title = {More Than One Replication Study Is Needed for Unambiguous Tests of
  Replication},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Journal of educational and behavioral statistics: a quarterly
  publication sponsored by the American Educational Research Association and the
  American Statistical Association},
  publisher = {American Educational Research Association},
  volume = {44},
  issue = {5},
  pages = {543-570},
  date = {2019-10-01},
  doi = {10.3102/1076998619852953},
  issn = {1076-9986},
  abstract = {The problem of assessing whether experimental results can be
  replicated is becoming increasingly important in many areas of science. It is
  often assumed that assessing replication is straightforward: All one needs to
  do is repeat the study and see whether the results of the original and
  replication studies agree. This article shows that the statistical test for
  whether two studies obtain the same effect is smaller than the power of either
  study to detect an effect in the first place. Thus, unless the original study
  and the replication study have unusually high power (e.g., power of 98\%), a
  single replication study will not have adequate sensitivity to provide an
  unambiguous evaluation of replication.},
  url = {https://doi.org/10.3102/1076998619852953},
  urldate = {2023-09-01},
  file = {Hedges and Schauer 2019 - More Than One Replication Study Is Needed for Unambiguous Tests of Replication.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Hedges2019-pr,
  title = {Consistency of effects is important in replication: Rejoinder to
  Mathur and VanderWeele (2019)},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {576-577},
  date = {2019-10},
  doi = {10.1037/met0000237},
  pmid = {31580142},
  issn = {1082-989X,1939-1463},
  abstract = {In this rejoinder, we discuss Mathur and VanderWeele's response to
  our article, "Statistical Analyses for Studying Replication: Meta-Analytic
  Perspectives," which appears in this current issue. We attempt to clarify a
  point of confusion regarding the inclusion of an original study in an analysis
  of replication, and the potential impact of publication bias. We then discuss
  the methods used by Mathur and VanderWeele to conduct an alternative analysis
  of the Gambler's Fallacy example from our article. We highlight that there are
  some potential statistical and conceptual differences to their approach
  compared to what we propose in our article. (PsycINFO Database Record (c) 2019
  APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000237},
  urldate = {2023-09-01},
  file = {Hedges and Schauer 2019 - Consistency of effects is important in replication - Rejoinder to Mathur and VanderWeele (2019).pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Schauer2021-bh,
  title = {An evaluation of statistical methods for aggregate patterns of
  replication failure},
  author = {Schauer, Jacob M and Fitzgerald, Kaitlyn G and Peko-Spicer, Sarah
  and Whalen, Mena C R and Zejnullahi, Rrita and Hedges, Larry V},
  journaltitle = {The Annals of Applied Statistics},
  publisher = {Institute of Mathematical Statistics},
  volume = {15},
  issue = {1},
  pages = {208-229},
  date = {2021-03},
  doi = {10.1214/20-AOAS1387},
  issn = {1932-6157,1941-7330},
  abstract = {Several programs of research have sought to assess the
  replicability of scientific findings in different fields, including economics
  and psychology. These programs attempt to replicate several findings and use
  the results to say something about large-scale patterns of replicability in a
  field. However, little work has been done to understand the analytic methods
  used to do this, including what they are assessing and what their statistical
  properties are. This article examines several methods that have been used to
  study patterns of replicability in the social sciences. We describe in
  concrete terms how each method operationalizes the idea of “replication” and
  examine various statistical properties, including bias, precision and
  statistical power. We find that some analytic methods rely on an operational
  definition of replication that can be misleading. Other methods involve more
  sound definitions of replication, but most of these have limitations, such as
  large bias and uncertainty or low power. The findings suggest that we should
  use caution interpreting the results of such analyses and that work on more
  accurate methods may be useful to future replication research efforts.},
  url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-15/issue-1/An-evaluation-of-statistical-methods-for-aggregate-patterns-of-replication/10.1214/20-AOAS1387.full},
  urldate = {2023-09-04},
  file = {Schauer et al. 2021 - An evaluation of statistical methods for aggregate patterns of replication failure.pdf},
  keywords = {bias; Meta-analysis; power; replication;;replication-methods},
  language = {en}
}

@ARTICLE{Hedges2021-of,
  title = {The Design of Replication Studies},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Journal of the Royal Statistical Society. Series A,},
  publisher = {Oxford Academic},
  volume = {184},
  issue = {3},
  pages = {868-886},
  date = {2021-03-31},
  doi = {10.1111/rssa.12688},
  issn = {0964-1998,1467-985X},
  abstract = {Abstract. Empirical evaluations of replication have become
  increasingly common, but there has been no unified approach to doing so. Some
  evaluations conduct onl},
  url = {https://academic.oup.com/jrsssa/article/184/3/868/7068411},
  urldate = {2023-09-01},
  file = {Hedges and Schauer 2021 - The Design of Replication Studies.pdf},
  keywords = {replication-methods},
  language = {en}
}

@INBOOK{Schauer2022-mj,
  title = {Replicability and Meta-Analysis},
  author = {Schauer, Jacob M},
  editor = {O'Donohue, William and Masuda, Akihiko and Lilienfeld, Scott},
  booktitle = {Avoiding Questionable Research Practices in Applied Psychology},
  publisher = {Springer International Publishing},
  location = {Cham},
  pages = {301-342},
  date = {2022},
  doi = {10.1007/978-3-031-04968-2_14},
  isbn = {9783031049682,9783031049682},
  abstract = {In this chapter, I will discuss statistical considerations for
  studying replication. More specifically, I will approach replication from a
  framework based on meta-analysis. To do so, I will focus on direct
  replications, where studies are designed to be as similar as possible, as
  opposed to conceptual replications that (systematically or haphazardly) vary
  in at least one aspect of an experiment. The chapter starts with a brief
  description of recent research on replication in psychology and uses examples
  from that research to highlight relevant considerations in defining and
  parametrizing “replication.” It then outlines different ways to frame analyses
  of replication and provides examples. Finally, it takes one possible
  definition of replication—that effects found across studies involving the same
  phenomenon are consistent—and describes relevant analyses and their
  properties.},
  url = {https://doi.org/10.1007/978-3-031-04968-2_14},
  urldate = {2023-09-01},
  file = {Schauer 2022 - Replicability and Meta-Analysis.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Schmidt2009-mq,
  title = {Shall we Really do it Again? The Powerful Concept of Replication is
  Neglected in the Social Sciences},
  author = {Schmidt, Stefan},
  journaltitle = {Review of general psychology: journal of Division 1, of the
  American Psychological Association},
  publisher = {SAGE Publications},
  volume = {13},
  issue = {2},
  pages = {90-100},
  date = {2009-06},
  doi = {10.1037/a0015108},
  issn = {1089-2680,1939-1552},
  abstract = {Replication is one of the most important tools for the
  verification of facts within the empirical sciences. A detailed examination of
  the notion of replication reveals that there are many different meanings to
  this concept and the relevant procedures, but hardly any systematic
  literature. This paper analyzes the concept of replication from a theoretical
  point of view. It demonstrates that the theoretical demands are scarcely met
  in everyday work within the social sciences. Some demands are just not
  feasible, whereas others are constricted by restrictions relating to
  publication. A new classification scheme based on a functional approach that
  distinguishes between different types of replication is proposed. Next, it
  will be argued that replication addresses the important connection between
  existing and new knowledge. To do so it has to be applied explicitly and
  systematically. The paper ends with a description of procedures how this could
  be done and a set of recommendations how to handle the concept of replication
  in the future to exploit its potential to the full.},
  url = {https://journals.sagepub.com/doi/abs/10.1037/a0015108},
  file = {Schmidt 2009 - Shall we Really do it Again - The Powerful Concept of Replication is Neglected in the Social Sciences.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Camerer2018-oj,
  title = {Evaluating the replicability of social science experiments in Nature
  and Science between 2010 and 2015},
  author = {Camerer, Colin F and Dreber, Anna and Holzmeister, Felix and Ho,
  Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and
  Nave, Gideon and Nosek, Brian A and Pfeiffer, Thomas and Altmejd, Adam and
  Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa,
  Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson,
  Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu,
  Hang},
  journaltitle = {Nature human behaviour},
  publisher = {Springer Science and Business Media LLC},
  volume = {2},
  issue = {9},
  pages = {637-644},
  date = {2018-09-27},
  doi = {10.1038/s41562-018-0399-z},
  pmid = {31346273},
  issn = {2397-3374,2397-3374},
  abstract = {Being able to replicate scientific findings is crucial for
  scientific progress1-15. We replicate 21 systematically selected experimental
  studies in the social sciences published in Nature and Science between 2010
  and 201516-36. The replications follow analysis plans reviewed by the original
  authors and pre-registered prior to the replications. The replications are
  high powered, with sample sizes on average about five times higher than in the
  original studies. We find a significant effect in the same direction as the
  original study for 13 (62\%) studies, and the effect size of the replications
  is on average about 50\% of the original effect size. Replicability varies
  between 12 (57\%) and 14 (67\%) studies for complementary replicability
  indicators. Consistent with these results, the estimated true-positive rate is
  67\% in a Bayesian analysis. The relative effect size of true positives is
  estimated to be 71\%, suggesting that both false positives and inflated effect
  sizes of true positives contribute to imperfect reproducibility. Furthermore,
  we find that peer beliefs of replicability are strongly related to
  replicability, suggesting that the research community could predict which
  results would replicate and that failures to replicate were not the result of
  chance alone.},
  url = {https://www.nature.com/articles/s41562-018-0399-z},
  urldate = {2023-07-31},
  file = {Camerer et al. 2018 - Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Valentine2011-yq,
  title = {Replication in prevention science},
  author = {Valentine, Jeffrey C and Biglan, Anthony and Boruch, Robert F and
  Castro, Felipe González and Collins, Linda M and Flay, Brian R and Kellam,
  Sheppard and Mościcki, Eve K and Schinke, Steven P},
  journaltitle = {Prevention science: the official journal of the Society for
  Prevention Research},
  publisher = {Springer Science and Business Media LLC},
  volume = {12},
  issue = {2},
  pages = {103-117},
  date = {2011-06-04},
  doi = {10.1007/s11121-011-0217-6},
  pmid = {21541692},
  issn = {1389-4986,1573-6695},
  abstract = {Replication research is essential for the advancement of any
  scientific field. In this paper, we argue that prevention science will be
  better positioned to help improve public health if (a) more replications are
  conducted; (b) those replications are systematic, thoughtful, and conducted
  with full knowledge of the trials that have preceded them; and (c)
  state-of-the art techniques are used to summarize the body of evidence on the
  effects of the interventions. Under real-world demands it is often not
  feasible to wait for multiple replications to accumulate before making
  decisions about intervention adoption. To help individuals and agencies make
  better decisions about intervention utility, we outline strategies that can be
  used to help understand the likely direction, size, and range of intervention
  effects as suggested by the current knowledge base. We also suggest structural
  changes that could increase the amount and quality of replication research,
  such as the provision of incentives and a more vigorous pursuit of prospective
  research registers. Finally, we discuss methods for integrating replications
  into the roll-out of a program and suggest that strong partnerships with local
  decision makers are a key component of success in replication research. Our
  hope is that this paper can highlight the importance of replication and
  stimulate more discussion of the important elements of the replication
  process. We are confident that, armed with more and better replications and
  state-of-the-art review methods, prevention science will be in a better
  position to positively impact public health.},
  url = {https://link.springer.com/article/10.1007/s11121-011-0217-6},
  urldate = {2023-07-31},
  file = {Valentine et al. 2011 - Replication in prevention science.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Schauer2020-tw,
  title = {Assessing heterogeneity and power in replications of psychological
  experiments},
  author = {Schauer, Jacob M and Hedges, Larry V},
  journaltitle = {Psychological bulletin},
  publisher = {American Psychological Association (APA)},
  volume = {146},
  issue = {8},
  pages = {701-719},
  date = {2020-08},
  doi = {10.1037/bul0000232},
  pmid = {32271029},
  issn = {0033-2909,1939-1455},
  abstract = {In this study, we reanalyze recent empirical research on
  replication from a meta-analytic perspective. We argue that there are
  different ways to define "replication failure," and that analyses can focus on
  exploring variation among replication studies or assess whether their results
  contradict the findings of the original study. We apply this framework to a
  set of psychological findings that have been replicated and assess the
  sensitivity of these analyses. We find that tests for replication that involve
  only a single replication study are almost always severely underpowered. Among
  the 40 findings for which ensembles of multisite direct replications were
  conducted, we find that between 11 and 17 (28\% to 43\%) ensembles produced
  heterogeneous effects, depending on how replication is defined. This
  heterogeneity could not be completely explained by moderators documented by
  replication research programs. We also find that these ensembles were not
  always well-powered to detect potentially meaningful values of heterogeneity.
  Finally, we identify several discrepancies between the results of original
  studies and the distribution of effects found by multisite replications but
  note that these analyses also have low power. We conclude by arguing that
  efforts to assess replication would benefit from further methodological work
  on designing replication studies to ensure analyses are sufficiently
  sensitive. (PsycInfo Database Record (c) 2020 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/bul0000232},
  urldate = {2023-07-31},
  file = {Schauer and Hedges 2020 - Assessing heterogeneity and power in replications of psychological experiments.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Schauer2021-ja,
  title = {Reconsidering statistical methods for assessing replication},
  author = {Schauer, J M and Hedges, L V},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {26},
  issue = {1},
  pages = {127-139},
  date = {2021-02},
  doi = {10.1037/met0000302},
  pmid = {33617275},
  issn = {1082-989X,1939-1463},
  abstract = {Recent empirical evaluations of replication in psychology have
  reported startlingly few successful replication attempts. At the same time,
  these programs have noted that the proper way to analyze replication studies
  is far from a settled matter and have analyzed their data in several different
  ways. This presents 2 challenges to interpreting the results of these
  programs. First, different analysis methods assess different operational
  definitions of replication. Second, the properties of these methods are not
  necessarily common knowledge; it is possible for a successful replication to
  be deemed a failure by nearly all of the metrics used, and it is not always
  immediately clear how likely such errors are to occur. In this article, we
  describe the methods commonly used in replication research and how they imply
  specific operational definitions of replication. We then compute the
  probability of false failure (i.e., a successful replication is concluded to
  have failed) and false success determinations. These are shown to be high
  (often over 50\%) and in many cases uncontrolled. We then demonstrate that
  errors are probable in the data to which these methods have been applied in
  the literature. We show that the probability that some reported conclusions
  about replication are incorrect can be as high as 75-80\%. (PsycInfo Database
  Record (c) 2021 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000302},
  urldate = {2023-07-31},
  file = {Schauer and Hedges 2021 - Reconsidering statistical methods for assessing replication.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Schauer2023-yn,
  title = {On the Accuracy of Replication Failure Rates},
  author = {Schauer, Jacob M},
  journaltitle = {Multivariate behavioral research},
  publisher = {Informa UK Limited},
  volume = {58},
  issue = {3},
  pages = {598-615},
  date = {2023},
  doi = {10.1080/00273171.2022.2066500},
  pmid = {37339430},
  issn = {0027-3171,1532-7906},
  abstract = {A prominent approach to studying the replication crisis has been
  to conduct replications of several different scientific findings as part of
  the same research effort. The reported proportion of findings that these
  programs determined failed to replicate have become important statistics in
  the replication crisis. However, these "failure rates" are based on decisions
  about whether individual studies replicated, which are themselves subject to
  statistical uncertainty. In this article, we examine how that uncertainty
  impacts the accuracy of reported failure rates and find that the reported
  failure rates can be substantially biased and highly variable. Indeed, very
  high or very low failure rates could arise from chance alone.},
  url = {http://dx.doi.org/10.1080/00273171.2022.2066500},
  urldate = {2023-07-31},
  keywords = {Replication; bias; decision theory; meta-analysis;
  uncertainty;replication-methods},
  language = {en}
}

@ARTICLE{Mathur2019-vh,
  title = {Challenges and suggestions for defining replication "success" when
  effects may be heterogeneous: Comment on Hedges and Schauer (2019)},
  author = {Mathur, Maya B and VanderWeele, Tyler J},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {571-575},
  date = {2019-10},
  doi = {10.1037/met0000223},
  pmc = {PMC6779319},
  pmid = {31580141},
  issn = {1082-989X,1939-1463},
  abstract = {Psychological scientists are now trying to replicate published
  research from scratch to confirm the findings. In an increasingly widespread
  replication study design, each of several collaborating sites (such as
  universities) independently tries to replicate an original study, and the
  results are synthesized across sites. Hedges and Schauer (2019) proposed
  statistical analyses for these replication projects; their analyses focus on
  assessing the extent to which results differ across the replication sites, by
  testing for heterogeneity among a set of replication studies, while excluding
  the original study. We agree with their premises regarding the limitations of
  existing analysis methods and regarding the importance of accounting for
  heterogeneity among the replications. This objective may be interesting in its
  own right. However, we argue that by focusing only on whether the replication
  studies have similar effect sizes to one another, these analyses are not
  particularly appropriate for assessing whether the replications in fact
  support the scientific effect under investigation or for assessing the power
  of multisite replication projects. We reanalyze Hedges and Schauer's (2019)
  example dataset using alternative metrics of replication success that directly
  address these objectives. We reach a more optimistic conclusion regarding
  replication success than they did, illustrating that the alternative metrics
  can lead to quite different conclusions from those of Hedges and Schauer
  (2019). (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000223},
  urldate = {2023-07-28},
  file = {Mathur and VanderWeele 2019 - Challenges and suggestions for defining re ... n effects may be heterogeneous - Comment on Hedges and Schauer (2019).pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Hedges2019-ry,
  title = {Statistical analyses for studying replication: Meta-analytic
  perspectives},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {557-570},
  date = {2019-10},
  doi = {10.1037/met0000189},
  pmid = {30070547},
  issn = {1082-989X,1939-1463},
  abstract = {Formal empirical assessments of replication have recently become
  more prominent in several areas of science, including psychology. These
  assessments have used different statistical approaches to determine if a
  finding has been replicated. The purpose of this article is to provide several
  alternative conceptual frameworks that lead to different statistical analyses
  to test hypotheses about replication. All of these analyses are based on
  statistical methods used in meta-analysis. The differences among the methods
  described involve whether the burden of proof is placed on replication or
  nonreplication, whether replication is exact or allows for a small amount of
  "negligible heterogeneity," and whether the studies observed are assumed to be
  fixed (constituting the entire body of relevant evidence) or are a sample from
  a universe of possibly relevant studies. The statistical power of each of
  these tests is computed and shown to be low in many cases, raising issues of
  the interpretability of tests for replication. (PsycINFO Database Record (c)
  2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000189},
  file = {Hedges and Schauer 2019 - Statistical analyses for studying replication - Meta-analytic perspectives.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Mathur2020-nw,
  title = {New statistical metrics for multisite replication projects},
  author = {Mathur, Maya B and VanderWeele, Tyler J},
  journaltitle = {Journal of the Royal Statistical Society. Series A,
  (Statistics in Society)},
  publisher = {Oxford University Press (OUP)},
  volume = {183},
  issue = {3},
  pages = {1145-1166},
  date = {2020-06-01},
  doi = {10.1111/rssa.12572},
  issn = {0964-1998,1467-985X},
  abstract = {Summary Increasingly, researchers are attempting to replicate
  published original studies by using large, multisite replication projects, at
  least 134 of which have been completed or are on going. These designs are
  promising to assess whether the original study is statistically consistent
  with the replications and to reassess the strength of evidence for the
  scientific effect of interest. However, existing analyses generally focus on
  single replications; when applied to multisite designs, they provide an
  incomplete view of aggregate evidence and can lead to misleading conclusions
  about replication success. We propose new statistical metrics representing
  firstly the probability that the original study's point estimate would be at
  least as extreme as it actually was, if in fact the original study were
  statistically consistent with the replications, and secondly the estimated
  proportion of population effects agreeing in direction with the original
  study. Generalized versions of the second metric enable consideration of only
  meaningfully strong population effects that agree in direction, or
  alternatively that disagree in direction, with the original study. These
  metrics apply when there are at least 10 replications (unless the
  heterogeneity estimate τ\textasciicircum=0, in which case the metrics apply
  regardless of the number of replications). The first metric assumes normal
  population effects but appears robust to violations in simulations; the second
  is distribution free. We provide R packages (Replicate and MetaUtility).},
  url = {https://academic.oup.com/jrsssa/article-abstract/183/3/1145/7056432},
  file = {Mathur and VanderWeele 2020 - New statistical metrics for multisite replication projects.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Simonsohn2015-kg,
  title = {Small telescopes: detectability and the evaluation of replication
  results},
  author = {Simonsohn, Uri},
  journaltitle = {Psychological science},
  publisher = {journals.sagepub.com},
  volume = {26},
  issue = {5},
  pages = {559-569},
  date = {2015-05},
  doi = {10.1177/0956797614567341},
  pmid = {25800521},
  issn = {0956-7976,1467-9280},
  abstract = {This article introduces a new approach for evaluating replication
  results. It combines effect-size estimation with hypothesis testing, assessing
  the extent to which the replication results are consistent with an effect size
  big enough to have been detectable in the original study. The approach is
  demonstrated by examining replications of three well-known findings. Its
  benefits include the following: (a) differentiating "unsuccessful" replication
  attempts (i.e., studies yielding p > .05) that are too noisy from those that
  actively indicate the effect is undetectably different from zero, (b)
  "protecting" true findings from underpowered replications, and (c) arriving at
  intuitively compelling inferences in general and for the revisited
  replications in particular.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0956797614567341},
  file = {Simonsohn 2015 - Small telescopes - detectability and the evaluation of replication results.pdf},
  keywords = {hypothesis testing; open materials; replication; statistical
  power;replication-methods},
  language = {en}
}

@ARTICLE{Errington2021-fb,
  title = {Investigating the replicability of preclinical cancer biology},
  author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and
  Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  journaltitle = {eLife},
  publisher = {eLife Sciences Publications, Ltd},
  volume = {10},
  date = {2021-12-07},
  doi = {10.7554/eLife.71601},
  pmc = {PMC8651293},
  pmid = {34874005},
  issn = {2050-084X},
  abstract = {Replicability is an important feature of scientific research, but
  aspects of contemporary research culture, such as an emphasis on novelty, can
  make replicability seem less important than it should be. The Reproducibility
  Project: Cancer Biology was set up to provide evidence about the replicability
  of preclinical research in cancer biology by repeating selected experiments
  from high-impact papers. A total of 50 experiments from 23 papers were
  repeated, generating data about the replicability of a total of 158 effects.
  Most of the original effects were positive effects (136), with the rest being
  null effects (22). A majority of the original effect sizes were reported as
  numerical values (117), with the rest being reported as representative images
  (41). We employed seven methods to assess replicability, and some of these
  methods were not suitable for all the effects in our sample. One method
  compared effect sizes: for positive effects, the median effect size in the
  replications was 85\% smaller than the median effect size in the original
  experiments, and 92\% of replication effect sizes were smaller than the
  original. The other methods were binary - the replication was either a success
  or a failure - and five of these methods could be used to assess both positive
  and null effects when effect sizes were reported as numerical values. For
  positive effects, 40\% of replications (39/97) succeeded according to three or
  more of these five methods, and for null effects 80\% of replications (12/15)
  were successful on this basis; combining positive and null effects, the
  success rate was 46\% (51/112). A successful replication does not definitively
  confirm an original finding or its theoretical interpretation. Equally, a
  failure to replicate does not disconfirm a finding, but it does suggest that
  additional investigation is needed to establish its reliability.},
  url = {http://dx.doi.org/10.7554/eLife.71601},
  file = {Errington et al. 2021 - Investigating the replicability of preclinical cancer biology.pdf},
  keywords = {Reproducibility Project: Cancer Biology; cancer biology;
  computational biology; credibility; human; meta-analysis; mouse; replication;
  reproducibility; reproducibility in cancer biology; systems biology;
  transparency;replication-methods},
  language = {en}
}
